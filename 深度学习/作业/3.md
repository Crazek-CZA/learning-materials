### 《深度学习平台与应用》作业三

1、分别用简短的语言描述一下单帧 CNN 模型，Early Fusion，Late Fusion，3D CNN的做法以及区别。

做法：
- 单帧CNN模型：训练一个普通的2D CNN模型对单个视频帧进行分类
- Early Fusion：对输入的视频帧首先进行reshape，将时序信息整合进通道中后，再使用一个2D卷积处理所有的时间信息
- Late Fusion：通过2D CNN处理视频的单个帧，之后将得到的所有特征concat后进一步进行分类
- 3D CNN：使用3D卷积核替代原先的2D卷积核，使网络在进行卷积时便能考虑到时间信息

区别：
&emsp;&emsp;单帧CNN模型完全**不考虑时序信息**，仅凭视频中单帧的图像信息进行分类；

&emsp;&emsp;而Early Fusion与Late Fusion分别在原始2D CNN模型的**不同阶段**增加考虑时序信息的模块：Early Fusion在处理输入的视频帧时首先进行reshape，将时序信息整合进通道中后，再使用一个2D卷积处理所有的时间信息；而Late Fusion则是在通过原始2D CNN处理输入视频的每个帧得到特征后，将得到的所有特征concat后进一步进行分类；两者将时序信息融入原始2D CNN模型的阶段不同，但是不同时间都有单独滤波器**不具有时间平移不变性**；

&emsp;&emsp;而3D CNN则是直接对原先网络的卷积核进行改进，通过增加卷积核的维度使其具有处理时序信息的能力，**具有时间平移不变性**。



2、如何使用RNN（如LSTM）建模视频中的长时间依赖？相比于3D CNN，这种方法有何特点？

方法：

1. 使用预训练的CNN对视频帧进行特征提取，将每帧转化为一个特征向量
2. 将每一帧的特征向量作为时间步输入到多层LSTM网络中，该多层LSTM网络在每一层使用不同的权重，跨时间共享权重，它将通过其门控机制对视频帧之间的时间依赖进行建模，每个时间步的LSTM会接收同一层前一时间步的隐藏状态与上一层相同时间步的输入，生成新的隐藏状态，以此捕捉视频中帧与帧之间的动态变化
3. 根据任务需求，LSTM的输出将通过全连接层或其他方式转化为目标标签

特点：

1. LSTM是以逐帧的方式对视频进行建模，而3D CNN通过3D卷积核在时间维度和空间维度上同时进行卷积
2. LSTM需要对输入的视频帧进行预处理以获得输入的特征向量，而3D CNN直接处理原始的帧
3. LSTM采用序列建模，并可以进行权重共享，计算开销较小，占用内存较少；而3D CNN因为采用了3D卷积核，计算开销与所需内存较大，
4. LSTM感受野较大，适合捕捉长时间依赖的动态变化，而3D CNN感受野较小，更适合短期的时空特征学习



3、结合课件中Spatio-Temporal Self-Attention (Nonlocal Block)部分的介绍，请补充下面的代码，

```python
import torch
import torch.nn as nn

class NonLocalBlock(nn.Module):
    def __init__(self, in_channels):
        super(NonLocalBlock, self).__init__()
        self.in_channels = in_channels
        self.inter_channels = in_channels // 2  # 缩减通道数，节约计算

        # 1x1x1 卷积层用于生成 Query, Key, Value
        self.query_conv = nn.Conv3d(in_channels, self.inter_channels, kernel_size=1)
        self.key_conv = nn.Conv3d(in_channels, self.inter_channels, kernel_size=1)
        self.value_conv = nn.Conv3d(in_channels, self.inter_channels, kernel_size=1)

        # 最终的输出通道映射回原始的 in_channels
        self.out_conv = nn.Conv3d(self.inter_channels, in_channels, kernel_size=1)

        # 用于归一化的 softmax
        self.softmax = nn.Softmax(dim=-1)

        # 初始化残差连接中的权重
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        # 输入维度: (N, C, T, H, W)
        batch_size, C, T, H, W = x.size()

        # 生成 Query, Key, Value
        query = self.query_conv(x).view(batch_size, self.inter_channels, -1)  # (N, C', T*H*W)
        key = self.key_conv(x).view(batch_size, self.inter_channels, -1)     # (N, C', T*H*W)
        value = self.value_conv(x).view(batch_size, self.inter_channels, -1) # (N, C', T*H*W)
        
        ####请在下面补充完整 Spatio-Temporal Self-Attention 计算过程

        
        
        

        return out

# 测试模块
if __name__ == "__main__":
    # 输入示例: Batch = 2, Channels = 64, Time = 8, Height = 32, Width = 32
    x = torch.rand(2, 64, 8, 32, 32)
    nonlocal_block = NonLocalBlock(in_channels=64)
    out = nonlocal_block(x)
    print("输入维度:", x.shape)
    print("输出维度:", out.shape)

```



4、 在风格迁移中，Gram矩阵的作用是什么？

作用：

- 在风格迁移中，Gram矩阵可以捕捉图像风格的统计特征
- 可以通过目标图像的Gram矩阵和源风格图像的Gram矩阵来计算风格损失，从而以最小化风格损失作为模型优化的目标
- 可以通过对多幅图像的Gram矩阵进行加权平均来混合风格


5、 风格迁移和快速风格迁移的区别？

风格迁移：

&emsp;&emsp;风格迁移通常使用卷积神经网络通过优化由风格损失与内容损失组成的目标函数，来训练网络生成输入图片风格迁移后的图像。

快速风格迁移：

&emsp;&emsp;快速风格迁移首先会针对指定的风格训练一个卷积神经网络，使其可以将输入图片的风格转换为指定风格，之后将所需图像通过一次前向传播直接生成风格迁移后的图像。

由上可知区别有：

- 风格迁移需要卷积神经网络的多次前向传播与后向传播才能完成，计算开销大，所需时间久；而快速风格迁移只需要一次前向传播就可以完成，计算开销小，速度快
- 风格迁移的网络可以使指定图像迁移至任意指定风格，而快速风格迁移在针对指定风格训练完之后仅具有对输入图像进行该风格迁移的能力



6、显著性图是怎么计算的（Saliency Map），它的用途是什么？

计算方式：

&emsp;&emsp;首先对输入图像进行前向传播，然后计算每个类预测值相对于图像像素的梯度，在 RGB 通道上取绝对值的最大值

用途：

- 可以用于无监督分割
- 可以用于发现数据偏差
- 可以用于生成对抗样本
- 有助于理解模型的决策过程
- 可以用于数据增强


7、请补充完整下面显著性可视化的代码

```python
import torch
import torchvision
import torchvision.transforms as T
import numpy as np
import matplotlib.pyplot as plt
import requests
from PIL import Image

def download(url,fname):
    response = requests.get(url)
    with open(fname,"wb") as f:
        f.write(response.content)

def preprocess(image, size=224):
    transform = T.Compose([
        T.Resize((size,size)),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        T.Lambda(lambda x: x[None]),
    ])
    return transform(image)
def deprocess(image):
    transform = T.Compose([
        T.Lambda(lambda x: x[0]),
        T.Normalize(mean=[0, 0, 0], std=[4.3668, 4.4643, 4.4444]),
        T.Normalize(mean=[-0.485, -0.456, -0.406], std=[1, 1, 1]),
        T.ToPILImage(),
    ])
    return transform(image)

def show_img(PIL_IMG):
    plt.imshow(np.asarray(PIL_IMG))

if __name__ =='__main__':
    model = torchvision.models.vgg19(pretrained=True)
    for param in model.parameters():
        param.requires_grad = False

    download("https://bkimg.cdn.bcebos.com/pic/3bf33a87e950352ac65cae81db13ecf2b21192131da3?x-bce-process=image/format,f_auto/quality,Q_70/resize,m_lfit,limit_1,w_536","input.jpg")
    img = Image.open('input.jpg') # 这里可以替换为自己的图片 

    X = preprocess(img) # X.shape: 1, 3, 224, 224
    model.eval()
    X.requires_grad_()
    output = model(X) # 1, 1000 
    print(output.shape)
    score_max_index = output.argmax()
    score_max = output[0, score_max_index]

    #####
    # 需要实现课件上显著性可视化部分，具体的步骤为：
    # a) 通过反向传播获取梯度
    # b) 在RGB通道上取梯度绝对值的最大值
    # 请在下面补充代码：
    
    
    
    #####


    plt.imshow(saliency[0], cmap=plt.cm.hot)
    plt.axis('off')
    plt.savefig("output.png")
```